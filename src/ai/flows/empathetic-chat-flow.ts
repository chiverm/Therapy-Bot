'use server';
/**
 * @fileOverview A Genkit flow to generate empathetic AI responses for a chat application.
 *
 * - generateEmpatheticResponse - A function that takes user input and conversation history to generate an AI response.
 * - EmpatheticChatInput - The input type for the generateEmpatheticResponse function.
 * - EmpatheticChatOutput - The return type for the generateEmpatheticResponse function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const EmpatheticChatInputSchema = z.object({
  userMessage: z.string().describe('The latest message from the user.'),
  formattedConversationHistory: z
    .string()
    .describe(
      'A string representing the past conversation. Each turn is prefixed with "User: " or "AI: ".'
    ),
});
export type EmpatheticChatInput = z.infer<typeof EmpatheticChatInputSchema>;

const EmpatheticChatOutputSchema = z.object({
  aiResponse: z.string().describe('The empathetic response generated by the AI.'),
});
export type EmpatheticChatOutput = z.infer<typeof EmpatheticChatOutputSchema>;

export async function generateEmpatheticResponse(
  input: EmpatheticChatInput
): Promise<EmpatheticChatOutput> {
  return empatheticChatFlow(input);
}

const prompt = ai.definePrompt({
  name: 'empatheticChatPrompt',
  input: {schema: EmpatheticChatInputSchema},
  output: {schema: EmpatheticChatOutputSchema},
  prompt: `You are DeepTalk, an AI companion designed for empathetic emotional support.
Your primary goal is to listen, understand, and offer supportive responses.
Avoid giving direct advice unless explicitly asked. Focus on validating feelings and encouraging reflection.
Keep your responses concise, natural, and aim for 1-3 sentences.

Here is the recent conversation history:
{{{formattedConversationHistory}}}

The user just said:
{{{userMessage}}}

Generate an empathetic response to the user.`,
});

const empatheticChatFlow = ai.defineFlow(
  {
    name: 'empatheticChatFlow',
    inputSchema: EmpatheticChatInputSchema,
    outputSchema: EmpatheticChatOutputSchema,
  },
  async input => {
    const {output} = await prompt(input);
    if (!output) {
      // Fallback in case the model doesn't return structured output as expected
      // or if the output is empty.
      console.warn('Empathetic chat flow did not produce expected output, returning default.');
      return { aiResponse: "I'm here to listen. Could you tell me a bit more?" };
    }
    return output;
  }
);
